{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b42eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KOMPUTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "####### Import Necessary Libraries #######\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "######### Utils #######\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "####### Data Loading && Preparation #######\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "# Use This for HF Dataset\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# Use This for Custom Model\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "####### Modelling #######\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#######Evaluation & Result #######\n",
    "from sklearn.metrics import classification_report, confusion_matrix,  accuracy_score, precision_recall_fscore_support\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35dbbbdf-17a6-4f74-b86f-8dc31de9d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a86f5a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aba3c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOOM_DATASET = '../../Dataset/learning_outcomes.csv'\n",
    "PAIR_DATASET = '../../Dataset/cpmk_subcpmk_pairs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fcfd193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Learning_Outcome     Jenis Level\n",
      "0     Mampu merencanakan, menyelesaikan, dan mengeva...      CPMK    C6\n",
      "1     Mampu merencanakan, menyelesaikan, dan mengeva...      CPMK    C6\n",
      "2     Mampu merencanakan, menyelesaikan, dan mengeva...      CPMK    C6\n",
      "3                           Mampu merancang gardu induk      CPMK    C6\n",
      "4                           Mampu merancang gardu induk      CPMK    C6\n",
      "...                                                 ...       ...   ...\n",
      "2571  Mahasiswa dapat memahami dan mempraktekkan iba...  Sub-CPMK    P3\n",
      "2572  Mampu menunjukan pelaksanaan penegakan hukum &...  Sub-CPMK    A5\n",
      "2573  Mahasiswa mampu menunjukkan implementasi nilai...  Sub-CPMK    A5\n",
      "2574  Mampu bertindak mengimplementasikan makna Sila...  Sub-CPMK    A5\n",
      "2575  Mampu bertindak mengimplementasikan Sila Ke-4 ...  Sub-CPMK    A5\n",
      "\n",
      "[2576 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpmk</th>\n",
       "      <th>level_cpmk</th>\n",
       "      <th>subcpmk</th>\n",
       "      <th>level_subcpmk</th>\n",
       "      <th>keselarasan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mampu merencanakan, menyelesaikan, dan mengeva...</td>\n",
       "      <td>C6</td>\n",
       "      <td>Merencanakan aplikasi menggunakan prinsip dasa...</td>\n",
       "      <td>C6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mampu merencanakan, menyelesaikan, dan mengeva...</td>\n",
       "      <td>C6</td>\n",
       "      <td>Menyelesaikan logic functions and gates</td>\n",
       "      <td>C2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mampu merencanakan, menyelesaikan, dan mengeva...</td>\n",
       "      <td>C6</td>\n",
       "      <td>Mengevaluasi boolean algebra dan combinational...</td>\n",
       "      <td>C5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mampu merancang gardu induk</td>\n",
       "      <td>C6</td>\n",
       "      <td>Mampu merancang instalasi listrik gardu induk</td>\n",
       "      <td>C6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mampu merancang gardu induk</td>\n",
       "      <td>C6</td>\n",
       "      <td>Mampu merancang sistem pengetanahan gardu induk</td>\n",
       "      <td>C6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>Mahasiswa mampu mempraktekkan haji, umroh dan ...</td>\n",
       "      <td>P3</td>\n",
       "      <td>Mahasiswa dapat memahami dan mempraktekkan iba...</td>\n",
       "      <td>P3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>Mampu menganalisis masalah kontekstual PKn, me...</td>\n",
       "      <td>C4</td>\n",
       "      <td>Mampu menunjukan pelaksanaan penegakan hukum &amp;...</td>\n",
       "      <td>A5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>Mahasiswa mampu menafsirkan dan menerapkan nil...</td>\n",
       "      <td>C3</td>\n",
       "      <td>Mahasiswa mampu menunjukkan implementasi nilai...</td>\n",
       "      <td>A5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>Mampu menunjukan nilai ketuhanan, nilai kemanu...</td>\n",
       "      <td>A5</td>\n",
       "      <td>Mampu bertindak mengimplementasikan makna Sila...</td>\n",
       "      <td>A5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>Mampu menunjukan nilai ketuhanan, nilai kemanu...</td>\n",
       "      <td>A5</td>\n",
       "      <td>Mampu bertindak mengimplementasikan Sila Ke-4 ...</td>\n",
       "      <td>A5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1288 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   cpmk level_cpmk  \\\n",
       "0     Mampu merencanakan, menyelesaikan, dan mengeva...         C6   \n",
       "1     Mampu merencanakan, menyelesaikan, dan mengeva...         C6   \n",
       "2     Mampu merencanakan, menyelesaikan, dan mengeva...         C6   \n",
       "3                           Mampu merancang gardu induk         C6   \n",
       "4                           Mampu merancang gardu induk         C6   \n",
       "...                                                 ...        ...   \n",
       "1283  Mahasiswa mampu mempraktekkan haji, umroh dan ...         P3   \n",
       "1284  Mampu menganalisis masalah kontekstual PKn, me...         C4   \n",
       "1285  Mahasiswa mampu menafsirkan dan menerapkan nil...         C3   \n",
       "1286  Mampu menunjukan nilai ketuhanan, nilai kemanu...         A5   \n",
       "1287  Mampu menunjukan nilai ketuhanan, nilai kemanu...         A5   \n",
       "\n",
       "                                                subcpmk level_subcpmk  \\\n",
       "0     Merencanakan aplikasi menggunakan prinsip dasa...            C6   \n",
       "1               Menyelesaikan logic functions and gates            C2   \n",
       "2     Mengevaluasi boolean algebra dan combinational...            C5   \n",
       "3         Mampu merancang instalasi listrik gardu induk            C6   \n",
       "4       Mampu merancang sistem pengetanahan gardu induk            C6   \n",
       "...                                                 ...           ...   \n",
       "1283  Mahasiswa dapat memahami dan mempraktekkan iba...            P3   \n",
       "1284  Mampu menunjukan pelaksanaan penegakan hukum &...            A5   \n",
       "1285  Mahasiswa mampu menunjukkan implementasi nilai...            A5   \n",
       "1286  Mampu bertindak mengimplementasikan makna Sila...            A5   \n",
       "1287  Mampu bertindak mengimplementasikan Sila Ke-4 ...            A5   \n",
       "\n",
       "      keselarasan  \n",
       "0            True  \n",
       "1            True  \n",
       "2            True  \n",
       "3            True  \n",
       "4            True  \n",
       "...           ...  \n",
       "1283         True  \n",
       "1284        False  \n",
       "1285        False  \n",
       "1286         True  \n",
       "1287         True  \n",
       "\n",
       "[1288 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom_df = pd.read_csv(BLOOM_DATASET)\n",
    "pair_df = pd.read_csv(PAIR_DATASET)\n",
    "\n",
    "print(bloom_df)\n",
    "pair_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a65fe",
   "metadata": {},
   "source": [
    "# Data Preparation & Modelling - Bloom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bafc28",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0ca62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks & cleaning\n",
    "required_cols = [\"Learning_Outcome\", \"Level\"]\n",
    "if not all(c in bloom_df.columns for c in required_cols):\n",
    "    raise ValueError(f\"CSV must contain columns: {required_cols}. Found: {list(bloom_df.columns)}\")\n",
    "\n",
    "bloom_df = bloom_df.dropna(subset=[\"Learning_Outcome\", \"Level\"]).reset_index(drop=True)\n",
    "bloom_df[\"Learning_Outcome\"] = bloom_df[\"Learning_Outcome\"].astype(str).str.strip()\n",
    "bloom_df[\"Level\"] = bloom_df[\"Level\"].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe3e5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping\n",
    "labels = sorted(bloom_df[\"Level\"].unique().tolist(), key=lambda x: (x[0], int(x[1:]) if x[1:].isdigit() else x))\n",
    "label2id = {lab:i for i, lab in enumerate(labels)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "\n",
    "bloom_df[\"label_id\"] = bloom_df[\"Level\"].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a594cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / val split (stratify by label)\n",
    "train_df, val_df = train_test_split(bloom_df, test_size=0.1, random_state=42, stratify=bloom_df[\"label_id\"])\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "datasets = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dd5f3",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bee272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDO_BERT_BASE = \"indobenchmark/indobert-large-p2\"   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d79ae-e074-479b-a3e3-cc660ffc8d57",
   "metadata": {},
   "source": [
    "### Single Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47679eda",
   "metadata": {},
   "source": [
    "#### Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81d17e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"Classification-result/indobert_single_head_out\"\n",
    "SEED = 42 \n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18e23e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707c78c70e846b6859c6b8e99562651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6d0fb9d0844c1c9f5229cc7dd97783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = INDO_BERT_BASE\n",
    "# Tokenizer and preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def preprocess_fn(batch):\n",
    "    enc = tokenizer(batch[\"Learning_Outcome\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    enc[\"labels\"] = batch[\"label_id\"]\n",
    "    return enc\n",
    "\n",
    "datasets = datasets.map(preprocess_fn, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032b7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(INDO_BERT_BASE , num_labels=num_labels, use_safetensors=True)\n",
    "model.to(device)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f31a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds_logits, labels = eval_pred\n",
    "    preds = np.argmax(preds_logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6479213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    fp16= True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5016379a-cbda-4e70-8692-39f6f06d654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "C:\\Users\\KOMPUTER\\anaconda3\\envs\\fadfad\\Lib\\site-packages\\accelerate\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "print(accelerate.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1d96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KOMPUTER\\AppData\\Local\\Temp\\ipykernel_21896\\1674331945.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fe65acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 42:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.334500</td>\n",
       "      <td>0.888088</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.731213</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.732518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.753544</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.768471</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.768532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.671947</td>\n",
       "      <td>0.786822</td>\n",
       "      <td>0.772296</td>\n",
       "      <td>0.786822</td>\n",
       "      <td>0.773398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.8383893287044832, metrics={'train_runtime': 2567.7458, 'train_samples_per_second': 2.708, 'train_steps_per_second': 0.169, 'total_flos': 6480913461645312.0, 'train_loss': 0.8383893287044832, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4319083c-4f9c-4fc7-af01-cec5f48903b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics: {'eval_loss': 0.6719468235969543, 'eval_accuracy': 0.7868217054263565, 'eval_precision': 0.7722959082261406, 'eval_recall': 0.7868217054263565, 'eval_f1': 0.7733983990917428, 'eval_runtime': 5.3087, 'eval_samples_per_second': 48.599, 'eval_steps_per_second': 3.202, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Eval final\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final evaluation metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6225eaf",
   "metadata": {},
   "source": [
    "#### Eval-Bloom Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f73626-5cc1-492d-8e68-e1bc08fd8719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0502396800453f87c5f3024afc7529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9d7f57932343b3b8049746c3783169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A3       0.00      0.00      0.00         1\n",
      "          A4       0.75      0.38      0.50         8\n",
      "          A5       0.56      0.62      0.59         8\n",
      "          C1       0.70      0.78      0.74         9\n",
      "          C2       0.82      0.84      0.83        49\n",
      "          C3       0.81      0.59      0.68        29\n",
      "          C4       0.83      0.88      0.85        49\n",
      "          C5       0.86      0.86      0.86        21\n",
      "          C6       0.88      0.98      0.93        47\n",
      "          P2       0.00      0.00      0.00         3\n",
      "          P3       0.60      0.82      0.69        22\n",
      "          P4       0.56      0.50      0.53        10\n",
      "          P5       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.79       258\n",
      "   macro avg       0.57      0.56      0.55       258\n",
      "weighted avg       0.77      0.79      0.77       258\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KOMPUTER\\anaconda3\\envs\\fadfad\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\KOMPUTER\\anaconda3\\envs\\fadfad\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\KOMPUTER\\anaconda3\\envs\\fadfad\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "def preprocess(ex):\n",
    "    return tokenizer(\n",
    "        ex[\"Learning_Outcome\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "val_tokenized = val_ds.map(preprocess, batched=True)\n",
    "val_tokenized = val_tokenized.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "    }\n",
    "\n",
    "loader = DataLoader(val_tokenized, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(model.device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(model.device)\n",
    "        }\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "decoded_preds = [id2label[i] for i in all_predictions]\n",
    "decoded_labels = [id2label[i] for i in all_labels]\n",
    "\n",
    "\n",
    "print(classification_report(decoded_labels, decoded_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74266a8d",
   "metadata": {},
   "source": [
    "#### Eval-Alignment Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36495c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./indobert_single_head_out\"   # tempat model & tokenizer disimpan\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "# nama kolom di dataframe\n",
    "CPMK_COL = \"cpmk\"\n",
    "SUBCPMK_COL = \"subcpmk\"\n",
    "GOLD_KES_COL = \"Keselarasan\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_texts(texts, batch_size=BATCH_SIZE):\n",
    "    preds = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Predicting\", leave=False):\n",
    "        batch_texts = [str(t) if t is not None else \"\" for t in texts[i:i+batch_size]]\n",
    "        enc = tokenizer(batch_texts, padding=\"longest\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
    "            preds.extend(batch_preds)\n",
    "    return preds\n",
    "\n",
    "def decode_ids_to_labels(id_list):\n",
    "    return [id2label.get(int(i), \"UNK\") for i in id_list]\n",
    "\n",
    "\n",
    "def pred_selaras(row):\n",
    "    dom_c, lvl_c = row[\"pred_domain_cpmk\"], row[\"pred_level_cpmk\"]\n",
    "    dom_s, lvl_s = row[\"pred_domain_sub\"], row[\"pred_level_sub\"]\n",
    "    # if any missing -> mark as Tidak Selaras (or choose None)\n",
    "    if dom_c is None or dom_s is None or lvl_c is None or lvl_s is None:\n",
    "        return \"Tidak Selaras\"\n",
    "    # domain must match and CPMK level must be >= Sub level\n",
    "    if dom_c != dom_s:\n",
    "        return \"Tidak Selaras\"\n",
    "    if lvl_c < lvl_s:\n",
    "        return \"Tidak Selaras\"\n",
    "    return \"Selaras\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[CPMK_COL] = df[CPMK_COL].fillna(\"\").astype(str)\n",
    "df[SUBCPMK_COL] = df[SUBCPMK_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# predict label ids\n",
    "pred_ids_cpmk = predict_texts(df[CPMK_COL].tolist(), batch_size=BATCH_SIZE)\n",
    "pred_ids_sub = predict_texts(df[SUBCPMK_COL].tolist(), batch_size=BATCH_SIZE)\n",
    "\n",
    "# decode to label strings\n",
    "pred_labels_cpmk = decode_ids_to_labels(pred_ids_cpmk)\n",
    "pred_labels_sub = decode_ids_to_labels(pred_ids_sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79819fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df[\"pred_label_cpmk\"] = pred_labels_cpmk\n",
    "df[\"pred_label_subcpmk\"] = pred_labels_sub\n",
    "\n",
    "\n",
    "df[\"pred_keselarasan\"] = df.apply(pred_selaras, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2207e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"level_cpmk\" in df.columns and \"level_subcpmk\" in df.columns:\n",
    "        # convert true columns to same format as model (e.g. 'C3')\n",
    "    true_labels_cpmk = df[\"level_cpmk\"].astype(str).str.upper().tolist()\n",
    "    true_labels_sub = df[\"level_subcpmk\"].astype(str).str.upper().tolist()\n",
    "\n",
    "if GOLD_KES_COL in df.columns:\n",
    "    gold = df[GOLD_KES_COL].astype(str).str.strip().replace({\"SELARAS\":\"Selaras\",\"TIDAK SELARAS\":\"Tidak Selaras\"})\n",
    "    pred = df[\"pred_keselarasan\"]\n",
    "    print(\"Keselarasan evaluation:\")\n",
    "    print(classification_report(gold, pred, zero_division=0))\n",
    "    print(\"Accuracy:\", accuracy_score(gold, pred))\n",
    "else:\n",
    "    print(f\"No gold keselarasan column named '{GOLD_KES_COL}' found — created predictions only.\")\n",
    "\n",
    "\n",
    "display(df[[CPMK_COL, \"pred_label_cpmk\", SUBCPMK_COL, \"pred_label_subcpmk\", \"pred_keselarasan\"]].sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79947fdd",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "690a542b-a06a-480c-a730-da982824aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer saved to Classification-result/indobert_single_head_out\n"
     ]
    }
   ],
   "source": [
    "# Save model, tokenizer, and label maps\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "pd.to_pickle(label2id, os.path.join(OUTPUT_DIR, \"label2id.pkl\"))\n",
    "pd.to_pickle(id2label, os.path.join(OUTPUT_DIR, \"id2label.pkl\"))\n",
    "print(\"Model & tokenizer saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40d8f8-f448-4782-82c5-da1177d6a16f",
   "metadata": {},
   "source": [
    "### Double Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d576e",
   "metadata": {},
   "source": [
    "#### Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6397b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 128   \n",
    "OUTPUT_DIR = \"Classification-result/indobert_double_head_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531088c8-5fe4-46b1-a021-2c377d0d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       C6\n",
       "1       C6\n",
       "2       C6\n",
       "3       C6\n",
       "4       C6\n",
       "        ..\n",
       "2571    P3\n",
       "2572    A5\n",
       "2573    A5\n",
       "2574    A5\n",
       "2575    A5\n",
       "Name: Level, Length: 2576, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom_df['Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "050c5587-b66a-45a7-a85e-fd08c986bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pecah CPMK menjadi data terpisah \n",
    "bloom_df['Domain'] = bloom_df['Level'].str[0]\n",
    "bloom_df['Aligned'] = bloom_df['Level'].str[1:].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b13cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for Domain and Level\n",
    "domain_classes = sorted(bloom_df['Domain'].unique().tolist())\n",
    "level_classes = sorted(bloom_df['Level'].unique().tolist(), key=lambda x: (x[0], int(x[1:]) if x[1:].isdigit() else x[1:]))\n",
    "\n",
    "domain2id = {c:i for i,c in enumerate(domain_classes)}\n",
    "id2domain = {i:c for c,i in domain2id.items()}\n",
    "\n",
    "level2id = {c:i for i,c in enumerate(level_classes)}\n",
    "id2level = {i:c for c,i in level2id.items()}\n",
    "\n",
    "bloom_df['domain_label'] = bloom_df['Domain'].map(domain2id)\n",
    "bloom_df['level_label'] = bloom_df['Level'].map(level2id)\n",
    "\n",
    "# Train/val split\n",
    "train_df, val_df = train_test_split(bloom_df, test_size=0.1, random_state=SEED, stratify=bloom_df['level_label'])\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e40ffe1-82ea-47cd-ae60-8a3ad936e89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f13cd7cea24d798c56a45611617f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50842942eaf43c1b58afff591052f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(INDO_BERT_BASE)\n",
    "\n",
    "def preprocess(batch):\n",
    "    enc = tokenizer(batch[\"Learning_Outcome\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    # map labels dari kolom yang sebelumnya kamu punya: 'domain_label' dan 'level_label'\n",
    "    enc[\"labels_domain\"] = batch[\"domain_label\"]\n",
    "    enc[\"labels_level\"]  = batch[\"level_label\"]\n",
    "    return enc\n",
    "\n",
    "datasets = datasets.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88895255-9e4b-417e-923b-f179d14a17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after map: ['Learning_Outcome', 'Jenis', 'Level', 'label_id', 'Domain', 'Aligned', 'domain_label', 'level_label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels_domain', 'labels_level']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns after map:\", datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f8b818-4c60-4e14-8541-e1d163dc7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_cols = [\"input_ids\", \"attention_mask\", \"labels_domain\", \"labels_level\"]\n",
    "datasets.set_format(type=\"torch\", columns=torch_cols)\n",
    "\n",
    "train_loader = DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(datasets[\"validation\"], batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "134c1d79-479f-4e16-bc15-3481d30a95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig, AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class MultiTaskConfig(PretrainedConfig):\n",
    "    model_type = \"indobert-multitask\"\n",
    "    def __init__(self, base_model_name_or_path=None, num_domain_labels=0, num_level_labels=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name_or_path = base_model_name_or_path\n",
    "        self.num_domain_labels = num_domain_labels\n",
    "        self.num_level_labels = num_level_labels\n",
    "\n",
    "class IndoBertForMultiTask(PreTrainedModel):\n",
    "    config_class = MultiTaskConfig\n",
    "\n",
    "    def __init__(self, config: MultiTaskConfig):\n",
    "        super().__init__(config)\n",
    "        # Use AutoModel (encoder only) so we reliably get last_hidden_state\n",
    "        # If you have memory issues, consider using a 'base' or 'small' checkpoint.\n",
    "        try:\n",
    "            self.bert = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path , num_labels=num_labels, use_safetensors=True)\n",
    "        except Exception as e:\n",
    "            # fallback: try without output_hidden_states\n",
    "            warnings.warn(f\"AutoModel.from_pretrained failed with: {e}. Retrying without output_hidden_states.\")\n",
    "            self.bert = AutoModel.from_pretrained(config.base_model_name_or_path, use_safetensors=True)\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # two classifier heads\n",
    "        self.classifier_domain = nn.Linear(hidden_size, config.num_domain_labels)\n",
    "        self.classifier_level  = nn.Linear(hidden_size, config.num_level_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels_domain=None, labels_level=None, return_dict=True, **kwargs):\n",
    "        # encoder forward\n",
    "        # allow token_type_ids to be None for models that don't use it\n",
    "        bert_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        if token_type_ids is not None:\n",
    "            bert_kwargs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "        outputs = self.bert(**bert_kwargs, return_dict=True)\n",
    "\n",
    "        # --- robust retrieval of pooled representation ---\n",
    "        # Prefer pooler_output if provided\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output\n",
    "        else:\n",
    "            # try last_hidden_state\n",
    "            last_hidden_state = None\n",
    "            if hasattr(outputs, \"last_hidden_state\") and outputs.last_hidden_state is not None:\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "            elif isinstance(outputs, tuple) and len(outputs) > 0:\n",
    "                # many HF models return tuple where [0] is last_hidden_state\n",
    "                last_hidden_state = outputs[0]\n",
    "            elif hasattr(outputs, \"hidden_states\") and outputs.hidden_states and len(outputs.hidden_states) > 0:\n",
    "                # use last entry of hidden_states\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "            if last_hidden_state is None:\n",
    "                raise ValueError(\n",
    "                    \"Cannot obtain last_hidden_state from encoder outputs. \"\n",
    "                    \"Make sure you loaded an encoder model (AutoModel) not a SequenceClassification model, \"\n",
    "                    \"or enable output_hidden_states in config.\"\n",
    "                )\n",
    "            pooled = last_hidden_state[:, 0, :]   # CLS token\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        logits_domain = self.classifier_domain(pooled)\n",
    "        logits_level  = self.classifier_level(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels_domain is not None and labels_level is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_domain = loss_fct(logits_domain.view(-1, self.config.num_domain_labels), labels_domain.view(-1))\n",
    "            loss_level  = loss_fct(logits_level.view(-1,  self.config.num_level_labels),  labels_level.view(-1))\n",
    "            loss = loss_domain + loss_level\n",
    "\n",
    "        # Build SequenceClassifierOutput (primary logits set to level logits)\n",
    "        seq_out = SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits_level,   # primary logits so Trainer compatibility preserved\n",
    "            hidden_states=getattr(outputs, \"hidden_states\", None),\n",
    "            attentions=getattr(outputs, \"attentions\", None),\n",
    "        )\n",
    "\n",
    "        # return tuple (seq_out, logits_domain, logits_level) for compatibility with custom trainer\n",
    "        return seq_out, logits_domain, logits_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99299edf-b64f-4986-9480-dae6116de25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "import os\n",
    "import math\n",
    "from typing import Optional, Dict, Any, List\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class MultiTaskTorchTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for multitask model with two heads (domain + level).\n",
    "    Expects model.forward(...) to return (seq_out, logits_domain, logits_level)\n",
    "    DataLoader must yield dicts containing:\n",
    "      - input_ids, attention_mask, (optional token_type_ids)\n",
    "      - labels_domain (torch.LongTensor), labels_level (torch.LongTensor)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 train_loader,\n",
    "                 val_loader=None,\n",
    "                 device: Optional[torch.device]=None,\n",
    "                 optimizer: Optional[torch.optim.Optimizer]=None,\n",
    "                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler]=None,\n",
    "                 num_epochs: int = 3,\n",
    "                 save_dir: str = \"./model_out\",\n",
    "                 clip_grad_norm: Optional[float] = None,\n",
    "                 loss_weights=(1.0, 1.0)):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.loss_weights = loss_weights\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _extract_logits(self, outputs):\n",
    "        # outputs expected (seq_out, logits_domain, logits_level)\n",
    "        if isinstance(outputs, tuple) and len(outputs) >= 3:\n",
    "            _, logits_domain, logits_level = outputs[:3]\n",
    "            return logits_domain, logits_level\n",
    "        if hasattr(outputs, \"logits_domain\") and hasattr(outputs, \"logits_level\"):\n",
    "            return outputs.logits_domain, outputs.logits_level\n",
    "        if isinstance(outputs, dict) and \"logits_domain\" in outputs and \"logits_level\" in outputs:\n",
    "            return outputs[\"logits_domain\"], outputs[\"logits_level\"]\n",
    "        # fallback: if single logits provided, use it for both (not recommended)\n",
    "        if hasattr(outputs, \"logits\"):\n",
    "            return outputs.logits, outputs.logits\n",
    "        raise ValueError(\"Model output format not recognized. Expect tuple(seq_out, logits_domain, logits_level).\")\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        pbar = tqdm(self.train_loader, desc=\"Train\", leave=False)\n",
    "        for batch in pbar:\n",
    "            # move tensors to device\n",
    "            batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "            labels_domain = batch.pop(\"labels_domain\", None)\n",
    "            labels_level  = batch.pop(\"labels_level\", None)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(**batch)  # returns (seq_out, logits_domain, logits_level)\n",
    "\n",
    "            logits_domain, logits_level = self._extract_logits(outputs)\n",
    "\n",
    "            if labels_domain is None or labels_level is None:\n",
    "                raise ValueError(\"Batch must include 'labels_domain' and 'labels_level'.\")\n",
    "\n",
    "            loss_domain = self.ce(logits_domain.view(-1, logits_domain.size(-1)), labels_domain.view(-1))\n",
    "            loss_level  = self.ce(logits_level.view(-1, logits_level.size(-1)), labels_level.view(-1))\n",
    "            loss = self.loss_weights[0]*loss_domain + self.loss_weights[1]*loss_level\n",
    "\n",
    "            loss.backward()\n",
    "            if self.clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler is not None:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        return total_loss / max(1, n_batches)\n",
    "\n",
    "    def validate(self):\n",
    "        if self.val_loader is None:\n",
    "            return None\n",
    "        self.model.eval()\n",
    "        preds_dom, preds_lvl, labs_dom, labs_lvl = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Val\", leave=False):\n",
    "                batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "                labels_domain = batch.pop(\"labels_domain\", None)\n",
    "                labels_level  = batch.pop(\"labels_level\", None)\n",
    "\n",
    "                outputs = self.model(**batch)\n",
    "                logits_domain, logits_level = self._extract_logits(outputs)\n",
    "\n",
    "                pred_d = torch.argmax(logits_domain, dim=-1).cpu().numpy().tolist()\n",
    "                pred_l = torch.argmax(logits_level, dim=-1).cpu().numpy().tolist()\n",
    "\n",
    "                preds_dom.extend(pred_d)\n",
    "                preds_lvl.extend(pred_l)\n",
    "                if labels_domain is not None:\n",
    "                    labs_dom.extend(labels_domain.cpu().numpy().tolist())\n",
    "                if labels_level is not None:\n",
    "                    labs_lvl.extend(labels_level.cpu().numpy().tolist())\n",
    "\n",
    "        metrics = {}\n",
    "        if len(labs_dom) > 0:\n",
    "            metrics[\"domain_acc\"] = accuracy_score(labs_dom, preds_dom)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(labs_dom, preds_dom, average=\"weighted\", zero_division=0)\n",
    "            metrics.update({\"domain_precision\": p, \"domain_recall\": r, \"domain_f1\": f1})\n",
    "        if len(labs_lvl) > 0:\n",
    "            metrics[\"level_acc\"] = accuracy_score(labs_lvl, preds_lvl)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(labs_lvl, preds_lvl, average=\"weighted\", zero_division=0)\n",
    "            metrics.update({\"level_precision\": p, \"level_recall\": r, \"level_f1\": f1})\n",
    "        return metrics\n",
    "\n",
    "    def fit(self):\n",
    "        best = -1.0\n",
    "        history = {\"train_loss\": [], \"val_metrics\": []}\n",
    "        for ep in range(self.num_epochs):\n",
    "            print(f\"Epoch {ep+1}/{self.num_epochs}\")\n",
    "            train_loss = self.train_epoch()\n",
    "            print(f\"  Train loss: {train_loss:.4f}\")\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            val_metrics = self.validate()\n",
    "            if val_metrics:\n",
    "                history[\"val_metrics\"].append(val_metrics)\n",
    "                print(f\"  Val metrics: {val_metrics}\")\n",
    "                # save best by level_f1 if present else domain_f1\n",
    "                score = val_metrics.get(\"level_f1\", val_metrics.get(\"domain_f1\", -1.0))\n",
    "                if score > best:\n",
    "                    best = score\n",
    "                    self.save(os.path.join(self.save_dir, \"best_model.pt\"))\n",
    "                    print(\"  Saved best model.\")\n",
    "            else:\n",
    "                # save last epoch if no val\n",
    "                self.save(os.path.join(self.save_dir, f\"model_epoch_{ep+1}.pt\"))\n",
    "        return history\n",
    "\n",
    "    def predict(self, loader):\n",
    "        self.model.eval()\n",
    "        preds_dom, preds_lvl, labels_dom, labels_lvl = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Predict\", leave=False):\n",
    "                batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "                labs_dom = batch.get(\"labels_domain\", None)\n",
    "                labs_lvl = batch.get(\"labels_level\", None)\n",
    "                # only pass input_* and attention_mask/token_type_ids\n",
    "                model_inputs = {k:v for k,v in batch.items() if k.startswith(\"input_\") or k in [\"attention_mask\",\"token_type_ids\"]}\n",
    "                outputs = self.model(**model_inputs)\n",
    "                logits_domain, logits_level = self._extract_logits(outputs)\n",
    "                preds_dom.extend(torch.argmax(logits_domain, dim=-1).cpu().numpy().tolist())\n",
    "                preds_lvl.extend(torch.argmax(logits_level, dim=-1).cpu().numpy().tolist())\n",
    "                if labs_dom is not None:\n",
    "                    labels_dom.extend(labs_dom.cpu().numpy().tolist())\n",
    "                if labs_lvl is not None:\n",
    "                    labels_lvl.extend(labs_lvl.cpu().numpy().tolist())\n",
    "        return {\"preds_domain\": preds_dom, \"preds_level\": preds_lvl, \"labels_domain\": labels_dom, \"labels_level\": labels_lvl}\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict() if self.optimizer is not None else None,\n",
    "                    \"scheduler_state_dict\": self.scheduler.state_dict() if self.scheduler is not None else None},\n",
    "                   path)\n",
    "\n",
    "    def load(self, path, map_location=None):\n",
    "        st = torch.load(path, map_location=map_location or self.device)\n",
    "        self.model.load_state_dict(st[\"model_state_dict\"])\n",
    "        if self.optimizer is not None and st.get(\"optimizer_state_dict\") is not None:\n",
    "            self.optimizer.load_state_dict(st[\"optimizer_state_dict\"])\n",
    "        if self.scheduler is not None and st.get(\"scheduler_state_dict\") is not None:\n",
    "            self.scheduler.load_state_dict(st[\"scheduler_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26e31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KOMPUTER\\AppData\\Local\\Temp\\ipykernel_12016\\2052874075.py:26: UserWarning: AutoModel.from_pretrained failed with: name 'num_labels' is not defined. Retrying without output_hidden_states.\n",
      "  warnings.warn(f\"AutoModel.from_pretrained failed with: {e}. Retrying without output_hidden_states.\")\n"
     ]
    }
   ],
   "source": [
    "config = MultiTaskConfig(base_model_name_or_path=INDO_BERT_BASE,\n",
    "                         num_domain_labels=len(domain2id),\n",
    "                         num_level_labels=len(level2id))\n",
    "model = IndoBertForMultiTask(config)\n",
    "\n",
    "\n",
    "# 3) optimizer & scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, total_iters=total_steps)\n",
    "\n",
    "# 4) trainer\n",
    "trainer = MultiTaskTorchTrainer(model=model,\n",
    "                                train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                optimizer=optimizer,\n",
    "                                scheduler=scheduler,\n",
    "                                device=torch.device(\"cuda\"),\n",
    "                                num_epochs=3,\n",
    "                                save_dir=OUTPUT_DIR,\n",
    "                                clip_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6870b8-46d1-4852-a07f-2ba34324eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54bde6740164805bcde2d884c498a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 1.8515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d1afb596cb4dc79e1c5ead8bff97fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val metrics: {'domain_acc': 0.9186046511627907, 'domain_precision': 0.9193599681971776, 'domain_recall': 0.9186046511627907, 'domain_f1': 0.9186939004487964, 'level_acc': 0.751937984496124, 'level_precision': 0.7423855188288666, 'level_recall': 0.751937984496124, 'level_f1': 0.7354216383631373}\n",
      "  Saved best model.\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4cfb3901fc48fda710c05fecaa2a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.7582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b741103341452f98d6d2d356b90275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val metrics: {'domain_acc': 0.9418604651162791, 'domain_precision': 0.9433148624867286, 'domain_recall': 0.9418604651162791, 'domain_f1': 0.9413039157225204, 'level_acc': 0.7674418604651163, 'level_precision': 0.744899202664768, 'level_recall': 0.7674418604651163, 'level_f1': 0.7527728259542278}\n",
      "  Saved best model.\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3487b7e7254486af391eefe087979a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.4405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939dffdb81e14f6291c908fabfe6bb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val metrics: {'domain_acc': 0.9302325581395349, 'domain_precision': 0.9306516252095336, 'domain_recall': 0.9302325581395349, 'domain_f1': 0.9273873961520503, 'level_acc': 0.7713178294573644, 'level_precision': 0.7383943982018376, 'level_recall': 0.7713178294573644, 'level_f1': 0.7509390707903746}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.8514908363079203, 0.75816957159289, 0.4404816364676788],\n",
       " 'val_metrics': [{'domain_acc': 0.9186046511627907,\n",
       "   'domain_precision': 0.9193599681971776,\n",
       "   'domain_recall': 0.9186046511627907,\n",
       "   'domain_f1': 0.9186939004487964,\n",
       "   'level_acc': 0.751937984496124,\n",
       "   'level_precision': 0.7423855188288666,\n",
       "   'level_recall': 0.751937984496124,\n",
       "   'level_f1': 0.7354216383631373},\n",
       "  {'domain_acc': 0.9418604651162791,\n",
       "   'domain_precision': 0.9433148624867286,\n",
       "   'domain_recall': 0.9418604651162791,\n",
       "   'domain_f1': 0.9413039157225204,\n",
       "   'level_acc': 0.7674418604651163,\n",
       "   'level_precision': 0.744899202664768,\n",
       "   'level_recall': 0.7674418604651163,\n",
       "   'level_f1': 0.7527728259542278},\n",
       "  {'domain_acc': 0.9302325581395349,\n",
       "   'domain_precision': 0.9306516252095336,\n",
       "   'domain_recall': 0.9302325581395349,\n",
       "   'domain_f1': 0.9273873961520503,\n",
       "   'level_acc': 0.7713178294573644,\n",
       "   'level_precision': 0.7383943982018376,\n",
       "   'level_recall': 0.7713178294573644,\n",
       "   'level_f1': 0.7509390707903746}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4384e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Eval - Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc5396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a076882",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dd35e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model, tokenizer, and label maps to: Classification-result/indobert_double_head_out\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(OUTPUT_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save label maps\n",
    "pd.to_pickle(domain2id, os.path.join(OUTPUT_DIR, \"Label Encoder-Decoder/domain2id.pkl\"))\n",
    "pd.to_pickle(id2domain, os.path.join(OUTPUT_DIR, \"Label Encoder-Decoder/id2domain.pkl\"))\n",
    "pd.to_pickle(level2id, os.path.join(OUTPUT_DIR, \"Label Encoder-Decoder/level2id.pkl\"))\n",
    "pd.to_pickle(id2level, os.path.join(OUTPUT_DIR, \"Label Encoder-Decoder/id2level.pkl\"))\n",
    "\n",
    "print(\"Saved model, tokenizer, and label maps to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935faff",
   "metadata": {},
   "source": [
    "# Data Preparation & Modelling - Siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1af18f-1b61-4c9d-8f34-80e5c2ff88df",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a953a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Keep only relevant columns (if missing, adjust)\n",
    "    needed = [\"cpmk\", \"subcpmk\", \"keselarasan\"]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Kolom '{c}' tidak ditemukan di df.\")\n",
    "    # drop rows with empty texts\n",
    "    df[\"cpmk\"] = df[\"cpmk\"].fillna(\"\").astype(str)\n",
    "    df[\"subcpmk\"] = df[\"subcpmk\"].fillna(\"\").astype(str)\n",
    "    # normalize keselarasan to binary labels: Selaras -> 1, else 0\n",
    "    def norm_kes(x):\n",
    "        s = str(x).strip().lower()\n",
    "        if s in [\"selaras\", \"1\", \"true\", \"ya\", \"yes\", \"True\"]:\n",
    "            return 1\n",
    "        return 0\n",
    "    df[\"label\"] = df[\"keselarasan\"].apply(norm_kes)\n",
    "    # optionally drop empty text rows\n",
    "    df = df[(df[\"cpmk\"].str.strip() != \"\") | (df[\"subcpmk\"].str.strip() != \"\")]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df[[\"cpmk\", \"subcpmk\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea2715",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbe3b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR =   \"Classification-result/indobert-siamese\"\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0b1d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTextDataset(Dataset):\n",
    "    def __init__(self, texts_a: List[str], texts_b: List[str], labels: List[int], tokenizer: AutoTokenizer, max_len=128):\n",
    "        self.texts_a = texts_a\n",
    "        self.texts_b = texts_b\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a = str(self.texts_a[idx])\n",
    "        b = str(self.texts_b[idx])\n",
    "        la = self.tokenizer(a, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        lb = self.tokenizer(b, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        item = {\n",
    "            \"input_ids_a\": la[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": la[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": lb[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": lb[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50f93453-9298-4f0d-8ad2-9df7cdcc4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch: list of items returned by __getitem__\n",
    "    input_ids_a = torch.stack([b[\"input_ids_a\"] for b in batch], dim=0)\n",
    "    attention_mask_a = torch.stack([b[\"attention_mask_a\"] for b in batch], dim=0)\n",
    "    input_ids_b = torch.stack([b[\"input_ids_b\"] for b in batch], dim=0)\n",
    "    attention_mask_b = torch.stack([b[\"attention_mask_b\"] for b in batch], dim=0)\n",
    "    labels = torch.stack([b[\"label\"] for b in batch], dim=0)\n",
    "    return {\n",
    "        \"input_ids_a\": input_ids_a,\n",
    "        \"attention_mask_a\": attention_mask_a,\n",
    "        \"input_ids_b\": input_ids_b,\n",
    "        \"attention_mask_b\": attention_mask_b,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a28519-7ce6-42e8-b1fd-a4fcd3122943",
   "metadata": {},
   "source": [
    "## Siamese modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ed04b47-d1ed-446c-b19b-b9debb2dcfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseIndoBert(nn.Module):\n",
    "    def __init__(self, model_name: str, dropout=0.1, hidden_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            use_safetensors=True,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        hs = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # classifier: take concatenation [hA, hB, |hA-hB|, hA*hB]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hs * 4, hs),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(hidden_dropout),\n",
    "            nn.Linear(hs, 1)   # output logits for binary\n",
    "        )\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        # return pooled CLS embedding\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        elif hasattr(out, \"last_hidden_state\"):\n",
    "            pooled = out.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            # fallback: hidden_states[-1][:,0,:] if available\n",
    "            if hasattr(out, \"hidden_states\") and out.hidden_states:\n",
    "                pooled = out.hidden_states[-1][:,0,:]\n",
    "            else:\n",
    "                raise ValueError(\"Encoder output has no pooler_output/last_hidden_state/hidden_states\")\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        hA = self.encode(input_ids_a, attention_mask_a)   # (bs, hs)\n",
    "        hB = self.encode(input_ids_b, attention_mask_b)\n",
    "        diff = torch.abs(hA - hB)\n",
    "        mul = hA * hB\n",
    "        feat = torch.cat([hA, hB, diff, mul], dim=-1)\n",
    "        feat = self.dropout(feat)\n",
    "        logits = self.classifier(feat).squeeze(-1)   # (bs,)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d7ab565-9bbc-46aa-9f68-cac1205ba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTrainer:\n",
    "    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader = None,\n",
    "                 lr=2e-5, epochs=3, device=None, weight_decay=0.01, warmup_steps=0):\n",
    "        self.model = model.to(DEVICE if device is None else device)\n",
    "        self.epochs = epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        self.device = DEVICE if device is None else device\n",
    "\n",
    "    def train_epoch(self, epoch_idx: int = 0):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch_idx+1}/{self.epochs} - Train\", leave=True)\n",
    "        for batch in pbar:\n",
    "            self.optimizer.zero_grad()\n",
    "            inputs = {k: v.to(self.device) for k,v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(self.device).float()\n",
    "            logits = self.model(**inputs)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # step scheduler after optimizer.step() to match HF Trainer behaviour\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # get current lr (for display)\n",
    "            try:\n",
    "                curr_lr = self.optimizer.param_groups[0]['lr']\n",
    "            except:\n",
    "                curr_lr = None\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{(total_loss / n_batches):.4f}\",\n",
    "                \"lr\": f\"{curr_lr:.2e}\" if curr_lr is not None else \"N/A\"\n",
    "            })\n",
    "\n",
    "        avg_loss = total_loss / max(1, n_batches)\n",
    "        pbar.close()\n",
    "        return avg_loss\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.val_loader is None:\n",
    "            return None\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\", leave=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                inputs = {k: v.to(self.device) for k,v in batch.items() if k != \"labels\"}\n",
    "                labels = batch[\"labels\"].to(self.device).float()\n",
    "                logits = self.model(**inputs)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "                # optionally show interim accuracy on the progressbar\n",
    "                if len(all_labels) > 0:\n",
    "                    interim_acc = accuracy_score(all_labels, all_preds)\n",
    "                    pbar.set_postfix({\"interim_acc\": f\"{interim_acc:.4f}\"})\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, [float(p) for p in all_preds])\n",
    "        except:\n",
    "            auc = None\n",
    "        return {\"report\": report, \"accuracy\": acc, \"auc\": auc}\n",
    "\n",
    "    def fit(self):\n",
    "        best_acc = -1.0\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self.train_epoch(epoch_idx=epoch)\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} - train_loss: {train_loss:.4f}\")\n",
    "            val_stats = self.evaluate()\n",
    "            if val_stats:\n",
    "                print(\"Validation accuracy:\", val_stats[\"accuracy\"])\n",
    "                print(val_stats[\"report\"])\n",
    "                # save best by accuracy\n",
    "                if val_stats[\"accuracy\"] > best_acc:\n",
    "                    best_acc = val_stats[\"accuracy\"]\n",
    "                    self.save(os.path.join(OUTPUT_DIR, \"best_model\"))\n",
    "                    print(\"Saved best model.\")\n",
    "        return\n",
    "\n",
    "    def predict_pairs(self, pairs: List[Tuple[str,str]], tokenizer: AutoTokenizer, batch_size=16):\n",
    "        self.model.eval()\n",
    "        texts_a = [a for a,b in pairs]\n",
    "        texts_b = [b for a,b in pairs]\n",
    "        ds = PairTextDataset(texts_a, texts_b, [0]*len(pairs), tokenizer, max_len=MAX_LEN)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        all_probs = []\n",
    "\n",
    "        pbar = tqdm(loader, desc=\"Predicting pairs\", leave=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                inputs = {k: v.to(self.device) for k,v in batch.items() if k != \"labels\"}\n",
    "                logits = self.model(**inputs)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().tolist()\n",
    "                all_probs.extend(probs)\n",
    "                # optional postfix\n",
    "                pbar.set_postfix({\"got\": len(all_probs)})\n",
    "        pbar.close()\n",
    "        return all_probs\n",
    "\n",
    "    def save(self, outdir):\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        # Save model as safe serialization (safetensors) if available\n",
    "        try:\n",
    "            model_to_save = getattr(self.model, \"module\", self.model)\n",
    "            encoder = model_to_save.encoder\n",
    "            encoder.save_pretrained(outdir, safe_serialization=True)\n",
    "            torch.save(self.model.classifier.state_dict(), os.path.join(outdir, \"classifier.pt\"))\n",
    "        except Exception as e:\n",
    "            torch.save(self.model.state_dict(), os.path.join(outdir, \"model_state_dict.pt\"))\n",
    "        print(\"Saved model to\", outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a63141-4eec-42f4-910b-9fd93adb2a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type train_ds: <class '__main__.PairTextDataset'>\n",
      "Sample item from PairTextDataset: dict_keys(['input_ids_a', 'attention_mask_a', 'input_ids_b', 'attention_mask_b', 'label'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f578f39390254bd19115631e986f2c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3 - Train:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load df (adjust path)\n",
    "    df_raw = pd.read_csv(\"../../Dataset/cpmk_subcpmk_pairs.csv\")   # must contain cpmk, subcpmk, keselarasan\n",
    "    df = prepare_df(df_raw)\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=SEED, stratify=df[\"label\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(INDO_BERT_BASE, use_fast=True)\n",
    "\n",
    "    train_ds = PairTextDataset(train_df[\"cpmk\"].tolist(), train_df[\"subcpmk\"].tolist(), train_df[\"label\"].tolist(), tokenizer, max_len=MAX_LEN)\n",
    "    val_ds   = PairTextDataset(val_df[\"cpmk\"].tolist(), val_df[\"subcpmk\"].tolist(), val_df[\"label\"].tolist(), tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "        # jika memakai PairTextDataset\n",
    "    print(\"Type train_ds:\", type(train_ds))\n",
    "    print(\"Sample item from PairTextDataset:\", train_ds[0].keys())\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = SiameseIndoBert(INDO_BERT_BASE)\n",
    "    trainer = SiameseTrainer(model, train_loader, val_loader, lr=LR, epochs=EPOCHS)\n",
    "    trainer.fit()\n",
    "\n",
    "    # example inference:\n",
    "    pairs = [(\"Teks CPMK contoh 1\", \"Teks Sub-CPMK contoh 1\"), (\"..\", \"..\")]\n",
    "    probs = trainer.predict_pairs(pairs, tokenizer)\n",
    "    print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d53bc-0bc2-40b4-8f2a-60d9e898ada2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b5f8c-d060-4c56-a587-c61a48c6cf72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fadfad-gpu)",
   "language": "python",
   "name": "fadfad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
